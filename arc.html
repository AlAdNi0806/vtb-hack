<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>Real-time Streaming ASR Frontend</title>
<style>
  body { 
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
    display: flex; 
    flex-direction: column; 
    align-items: center; 
    background-color: #f0f2f5;
    color: #333;
    padding: 2em;
  }
  h1 {
    color: #111;
  }
  #controls {
    display: flex;
    align-items: center;
    gap: 1em;
  }
  #status {
    font-style: italic;
    color: #666;
  }
  #transcript-container {
    border: 1px solid #ccc;
    margin-top: 20px;
    width: 80%;
    max-width: 800px;
    background-color: #ffffff;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
  }
  #transcript {
    padding: 15px;
    min-height: 150px;
    white-space: pre-wrap; /* Allows text to wrap */
    word-wrap: break-word;
    font-size: 1.1em;
    line-height: 1.5;
  }
  button { 
    padding: 12px 24px; 
    font-size: 1.1em; 
    cursor: pointer; 
    border-radius: 6px;
    border: none;
    color: white;
    background-color: #007bff;
    transition: background-color 0.2s ease;
  }
  button:hover {
    background-color: #0056b3;
  }
  button.recording {
    background-color: #dc3545;
  }
  button.recording:hover {
    background-color: #c82333;
  }
</style>
</head>
<body>

<h1>Real-time Speech-to-Text üéôÔ∏è</h1>
<div id="controls">
    <button id="startStopBtn">Start Recording</button>
    <div id="status">Status: Idle</div>
</div>

<div id="transcript-container">
    <div id="transcript" placeholder="Transcription appears here..."></div>
</div>

<script>
const startStopBtn = document.getElementById('startStopBtn');
const transcriptDiv = document.getElementById('transcript');
const statusDiv = document.getElementById('status');
// ‚ñº‚ñº‚ñº IMPORTANT: Change this to your server's local IP address ‚ñº‚ñº‚ñº
const SERVER_URL = 'ws://localhost:8765';

let audioCtx;
let source;
let workletNode;
let socket;
let isRecognizing = false;
let finalTranscript = ''; // Stores the finalized part of the transcript

// --- Main control function ---
startStopBtn.onclick = () => {
  if (isRecognizing) {
    stopRecording();
  } else {
    startRecording();
  }
};

// --- Start Recording Logic ---
async function startRecording() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    source = audioCtx.createMediaStreamSource(stream);

    socket = new WebSocket(SERVER_URL);
    
    // Clear previous transcript
    finalTranscript = ''; 
    transcriptDiv.textContent = '';
    
    socket.onopen = async () => {
      console.log("WebSocket connection opened.");
      statusDiv.textContent = 'Status: Connected';
      
      // Load the AudioWorklet module
      await audioCtx.audioWorklet.addModule('resampler-processor.js');

      // Create the AudioWorkletNode for resampling
      workletNode = new AudioWorkletNode(audioCtx, 'resampler-processor', {
        processorOptions: {
          nativeSampleRate: audioCtx.sampleRate,
          targetSampleRate: 16000
        }
      });

      // Listen for messages (processed audio) from the worklet
      workletNode.port.onmessage = (event) => {
        if (socket.readyState === WebSocket.OPEN) {
          // Send the Int16 PCM data to the server
          socket.send(event.data.buffer); 
        }
      };

      // Connect the audio graph: Mic Source -> Worklet
      source.connect(workletNode);
      
      startStopBtn.textContent = 'Stop Recording';
      startStopBtn.classList.add('recording');
      transcriptDiv.textContent = 'Listening...';
      isRecognizing = true;
    };
    
    // ‚ñº‚ñº‚ñº THIS IS THE UPDATED LOGIC ‚ñº‚ñº‚ñº
    socket.onmessage = (event) => {
        const data = JSON.parse(event.data);
        if (data.transcript) {
            if (data.is_final) {
                // This is a final transcript. Append it to our stable string.
                finalTranscript += data.transcript + ' ';
                // Display the finalized transcript.
                transcriptDiv.textContent = finalTranscript;
            } else {
                // This is an interim transcript. Display it temporarily.
                // We show the stable `finalTranscript` plus the current interim one.
                transcriptDiv.textContent = finalTranscript + data.transcript;
            }
        }
    };


    socket.onclose = () => {
      console.log("WebSocket connection closed.");
      statusDiv.textContent = 'Status: Disconnected';
      stopRecording(); // Ensure cleanup happens
    };

    socket.onerror = (error) => {
      console.error("WebSocket Error:", error);
      statusDiv.textContent = 'Status: Error';
      stopRecording();
    };

  } catch (error) {
    console.error("Error starting recording:", error);
    alert("Could not start recording. Please grant microphone permissions and ensure you are running on localhost or HTTPS.");
  }
}

// --- Stop Recording Logic ---
function stopRecording() {
  if (socket && socket.readyState === WebSocket.OPEN) {
    // We don't need to send a "stop" message as the backend relies on VAD
    socket.close();
  }
  
  if (source) {
    source.disconnect();
    source.mediaStream.getTracks().forEach(track => track.stop());
  }

  if (workletNode) {
    workletNode.disconnect();
  }
  
  if (audioCtx && audioCtx.state !== 'closed') {
    audioCtx.close();
  }
  
  statusDiv.textContent = 'Status: Idle';
  startStopBtn.textContent = 'Start Recording';
  startStopBtn.classList.remove('recording');
  isRecognizing = false;
}
</script>
</body>
</html>